# -*- coding: utf-8 -*-
"""Wearable-Based Stress Detection Using Fitbit Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kh35tPk7hTH4JIOAtHifUMdu6UKdMsxP
"""

# Step 1: download the LifeSnaps zip from Zenodo directly in Colab
!wget "https://zenodo.org/records/7229547/files/rais_anonymized.zip?download=1" -O rais_anonymized.zip

# Unzip into a folder called "rais_anonymized"
!unzip -q rais_anonymized.zip -d rais_anonymized

# Check what we got
!ls
!ls rais_anonymized

# Check contents
!ls
!ls rais_anonymized
!ls rais_anonymized/csv_rais_anonymized

!ls rais_anonymized

# Check exact folder and file names

!pwd
print("\nTop-level:")
!ls

print("\nInside rais_anonymized:")
!ls rais_anonymized

print("\nInside rais_anonymized/csv_rais_anonymized:")
!ls rais_anonymized/csv_rais_anonymized

import pandas as pd

print("Inside rais_anonymized:")
!ls rais_anonymized

print("\nInside inner rais_anonymized:")
!ls rais_anonymized/rais_anonymized

# Try to load the daily Fitbit CSV from the nested folder
daily_path = "rais_anonymized/rais_anonymized/csv_rais_anonymized/daily_fitbit_sema_df_unprocessed.csv"
df_daily = pd.read_csv(daily_path)

print("\nDaily shape:", df_daily.shape)

print("\nFirst 5 rows:")
print(df_daily.head())

print("\nColumn names:")
for i, c in enumerate(df_daily.columns):
    print(i, c)

import numpy as np

# 1) choose node features (you can change this list later if needed)
node_features = [
    "nightly_temperature",
    "nremhr",
    "rmssd",
    "stress_score",
    "calories",
    "distance",
    "resting_hr",
    "sleep_duration",
    "sleep_efficiency",
    "steps",
    "sedentary_minutes",
    "very_active_minutes",
]

print("Using", len(node_features), "nodes:")
print(node_features)

# 2) keep only rows where all these columns are present
X = df_daily[node_features].dropna()
print("\nRows after dropping NaNs:", X.shape[0])

# 3) correlation matrix (K x K)
corr = X.corr().values
K = corr.shape[0]

# 4) build adjacency from absolute correlation
A = np.abs(corr)
np.fill_diagonal(A, 0.0)

# threshold weak connections
threshold = 0.3  # can tune later
A[A < threshold] = 0.0

# 5) row-normalize adjacency (for later GNN use)
row_sums = A.sum(axis=1, keepdims=True) + 1e-8
A_norm = A / row_sums

# 6) show adjacency matrix
adj_df = pd.DataFrame(A, index=node_features, columns=node_features)
print("\nAdjacency matrix (after thresholding):")
print(adj_df.round(3))

import networkx as nx
import matplotlib.pyplot as plt

# Build graph from adjacency
G = nx.from_numpy_array(A)
mapping = dict(enumerate(node_features))
G = nx.relabel_nodes(G, mapping)

plt.figure(figsize=(7, 7))

# Layout for node positions
pos = nx.spring_layout(G, seed=42)

# Draw nodes and edges
nx.draw_networkx_nodes(G, pos, node_size=900)
nx.draw_networkx_edges(G, pos, width=2)
nx.draw_networkx_labels(G, pos, font_size=9)

plt.title("ECGN Network Map (LifeSnaps Fitbit daily)")
plt.axis("off")
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1) Features for Method 1 (same as our ECGN nodes, but no graph here)
feat_cols = [
    "nightly_temperature",
    "nremhr",
    "rmssd",
    "calories",
    "distance",
    "resting_hr",
    "sleep_duration",
    "sleep_efficiency",
    "steps",
    "sedentary_minutes",
    "very_active_minutes",
]

# Build dataframe with only needed columns
cols_needed = feat_cols + ["stress_score"]
df_m1 = df_daily[cols_needed].dropna()

print("Rows after dropna (Method 1 dataset):", df_m1.shape[0])

# 2) Binary label: 0 = low stress, 1 = high stress (median split)
stress_vals = df_m1["stress_score"].values
stress_threshold = np.median(stress_vals)
y_all = (stress_vals >= stress_threshold).astype(int)

print("Stress median (threshold):", stress_threshold)
unique, counts = np.unique(y_all, return_counts=True)
print("Label counts {class: n}:", dict(zip(unique, counts)))

# 3) Feature matrix
X_all = df_m1[feat_cols].values

# Standardize features
scaler_m1 = StandardScaler()
X_scaled = scaler_m1.fit_transform(X_all)

print("X_scaled shape:", X_scaled.shape)

# 4) Train/val/test split: 60/20/20, stratified
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X_scaled, y_all, test_size=0.2, random_state=42, stratify=y_all
)
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval
)

print("Train size:", X_train.shape[0])
print("Val size:", X_val.shape[0])
print("Test size:", X_test.shape[0])

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix

# 1) Define and train the model
log_reg = LogisticRegression(max_iter=1000, class_weight="balanced", random_state=42)
log_reg.fit(X_train, y_train)

# 2) Evaluate on train / val / test
def eval_split(name, X, y):
    y_pred = log_reg.predict(X)
    acc = accuracy_score(y, y_pred)
    f1 = f1_score(y, y_pred)
    print(f"{name} accuracy: {acc:.3f}, F1: {f1:.3f}")
    return y_pred

print("=== Logistic Regression (Method 1 baseline) ===")
y_train_pred = eval_split("Train", X_train, y_train)
y_val_pred   = eval_split("Val",   X_val,   y_val)
y_test_pred  = eval_split("Test",  X_test,  y_test)

print("\nTest classification report:")
print(classification_report(y_test, y_test_pred, digits=3))

print("Test confusion matrix:")
print(confusion_matrix(y_test, y_test_pred))

# === Method 2: GCN on ECGN graph (GCNN-style) ===

import torch
import torch.nn as nn
import torch.optim as optim

# 1) Rebuild adjacency A_gcn for the 11 node features (using full df_m1)
feat_cols_m2 = [
    "nightly_temperature",
    "nremhr",
    "rmssd",
    "calories",
    "distance",
    "resting_hr",
    "sleep_duration",
    "sleep_efficiency",
    "steps",
    "sedentary_minutes",
    "very_active_minutes",
]

X_corr = df_m1[feat_cols_m2]  # same df_m1 from Method 1

corr_m2 = X_corr.corr().values              # (11 x 11)
A_m2 = np.abs(corr_m2)
np.fill_diagonal(A_m2, 0.0)

edge_threshold_m2 = 0.3
A_m2[A_m2 < edge_threshold_m2] = 0.0

# Row-normalized adjacency (A_hat) for GCN
row_sums_m2 = A_m2.sum(axis=1, keepdims=True) + 1e-8
A_norm_m2 = A_m2 / row_sums_m2

print("Adjacency for Method 2 (A_m2):")
print(pd.DataFrame(A_m2, index=feat_cols_m2, columns=feat_cols_m2).round(3))

# 2) Convert adjacency to torch tensor
A_torch = torch.tensor(A_norm_m2, dtype=torch.float32)

# 3) Prepare feature tensors for GCN: (N, num_nodes, in_dim=1)
def to_node_tensor(X_np):
    # X_np: (N, 11)
    return torch.tensor(X_np, dtype=torch.float32).unsqueeze(-1)  # -> (N, 11, 1)

X_train_g = to_node_tensor(X_train)
X_val_g   = to_node_tensor(X_val)
X_test_g  = to_node_tensor(X_test)

y_train_t = torch.tensor(y_train, dtype=torch.long)
y_val_t   = torch.tensor(y_val, dtype=torch.long)
y_test_t  = torch.tensor(y_test, dtype=torch.long)

print("\nX_train_g shape (N, num_nodes, in_dim):", X_train_g.shape)
print("X_val_g shape:", X_val_g.shape)
print("X_test_g shape:", X_test_g.shape)
print("A_torch shape:", A_torch.shape)

# === Method 2: GCN on ECGN graph (GCNN-style) ===

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Move data and adjacency to torch tensors (if not already)
A_g = A_torch.to(device)  # (11, 11)

X_train_g_t = X_train_g.to(device)  # (N_train, 11, 1)
X_val_g_t   = X_val_g.to(device)
X_test_g_t  = X_test_g.to(device)

y_train_t = y_train_t.to(device)
y_val_t   = y_val_t.to(device)
y_test_t  = y_test_t.to(device)

# DataLoaders
batch_size = 64
train_ds = TensorDataset(X_train_g_t, y_train_t)
val_ds   = TensorDataset(X_val_g_t,   y_val_t)
test_ds  = TensorDataset(X_test_g_t,  y_test_t)

train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)
test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)

# Simple GCN: A * X -> Linear -> ReLU -> A * H -> Linear -> ReLU -> mean over nodes -> classifier
class SimpleGCN(nn.Module):
    def __init__(self, in_dim, hidden_dim, num_classes):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, num_classes)

    def forward(self, X, A):
        # X: (B, N, F), A: (N, N)
        # 1st graph conv
        H = torch.matmul(A, X)           # (B, N, F)
        H = self.fc1(H)                  # (B, N, hidden_dim)
        H = torch.relu(H)

        # 2nd graph conv
        H = torch.matmul(A, H)           # (B, N, hidden_dim)
        H = self.fc2(H)                  # (B, N, hidden_dim)
        H = torch.relu(H)

        # Global mean pooling over nodes -> (B, hidden_dim)
        H_pool = H.mean(dim=1)

        # Classification layer -> (B, num_classes)
        logits = self.out(H_pool)
        return logits

# Model, loss, optimizer
in_dim = 1
hidden_dim = 32
num_classes = 2

gcn_model = SimpleGCN(in_dim=in_dim, hidden_dim=hidden_dim, num_classes=num_classes).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(gcn_model.parameters(), lr=1e-3)

# Training loop
def run_epoch(model, loader, train=True):
    if train:
        model.train()
    else:
        model.eval()

    all_preds = []
    all_labels = []
    total_loss = 0.0

    with torch.set_grad_enabled(train):
        for X_batch, y_batch in loader:
            logits = model(X_batch, A_g)
            loss = criterion(logits, y_batch)

            if train:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            total_loss += loss.item() * X_batch.size(0)

            preds = logits.argmax(dim=1)
            all_preds.append(preds.detach().cpu())
            all_labels.append(y_batch.detach().cpu())

    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)

    avg_loss = total_loss / len(loader.dataset)
    return avg_loss, acc, f1

# Train for a few epochs
num_epochs = 50
best_val_f1 = 0.0
best_state = None

for epoch in range(1, num_epochs + 1):
    train_loss, train_acc, train_f1 = run_epoch(gcn_model, train_loader, train=True)
    val_loss, val_acc, val_f1 = run_epoch(gcn_model, val_loader, train=False)

    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        best_state = gcn_model.state_dict()

    if epoch % 10 == 0 or epoch == 1:
        print(
            f"Epoch {epoch:02d} | "
            f"Train loss {train_loss:.3f}, acc {train_acc:.3f}, f1 {train_f1:.3f} | "
            f"Val loss {val_loss:.3f}, acc {val_acc:.3f}, f1 {val_f1:.3f}"
        )

# Load best model on val
if best_state is not None:
    gcn_model.load_state_dict(best_state)

# Final evaluation on train / val / test
def eval_loader(name, loader):
    loss, acc, f1 = run_epoch(gcn_model, loader, train=False)
    print(f"{name} loss {loss:.3f}, acc {acc:.3f}, f1 {f1:.3f}")

print("\n=== Method 2: GCN on ECGN graph (final performance) ===")
eval_loader("Train", train_loader)
eval_loader("Val",   val_loader)
eval_loader("Test",  test_loader)

# === Method 3: Temporal GNN (GCN + GRU, eeg-gnn-ssl inspired) ===

import numpy as np
import pandas as pd

# We reuse feat_cols and stress_threshold from Method 1
print("Feature columns (nodes):", feat_cols)
print("Using stress_threshold from Method 1:", stress_threshold)

# 1) Build base dataframe with id, date, features, and label
seq_cols = ["id", "date"] + feat_cols + ["stress_score"]
df_seq = df_daily[seq_cols].dropna().copy()

# Ensure date is datetime and sort
df_seq["date"] = pd.to_datetime(df_seq["date"])
df_seq = df_seq.sort_values(["id", "date"])

# Binary label for sequences: 0 = low stress, 1 = high stress
df_seq["label_bin"] = (df_seq["stress_score"].values >= stress_threshold).astype(int)

print("Total rows after cleaning for sequences:", df_seq.shape[0])

# 2) Build sliding 7-day windows per user
T = 7  # window length in days
X_seq = []
y_seq = []

for uid, grp in df_seq.groupby("id"):
    grp = grp.sort_values("date")
    values = grp[feat_cols].values      # shape (n_days, K)
    labels = grp["label_bin"].values    # shape (n_days,)
    n = len(grp)
    if n < T:
        continue  # not enough days for a window

    for t in range(T - 1, n):
        window = values[t - T + 1 : t + 1]  # last T days -> shape (T, K)
        X_seq.append(window)
        y_seq.append(labels[t])             # label = stress on last day

X_seq = np.array(X_seq)   # (N_seq, T, K)
y_seq = np.array(y_seq)   # (N_seq,)

print("X_seq shape (N_seq, T, K):", X_seq.shape)
print("y_seq shape:", y_seq.shape)
unique, counts = np.unique(y_seq, return_counts=True)
print("Label counts in sequence dataset:", dict(zip(unique, counts)))

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# === Method 3: preprocessing for temporal GNN ===

N_seq, T, K = X_seq.shape
print("Seq shape before scaling:", X_seq.shape)

# 1) Flatten over time so scaler sees all timesteps
X_flat = X_seq.reshape(-1, K)  # (N_seq * T, K)

scaler_m3 = StandardScaler()
X_flat_scaled = scaler_m3.fit_transform(X_flat)

# 2) Reshape back to (N_seq, T, K)
X_seq_scaled = X_flat_scaled.reshape(N_seq, T, K)
print("Seq shape after scaling:", X_seq_scaled.shape)

# 3) Train/val/test split: 60/20/20, stratified by y_seq
X_train_seqval, X_test_seq, y_train_seqval, y_test_seq = train_test_split(
    X_seq_scaled, y_seq, test_size=0.2, random_state=42, stratify=y_seq
)
X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split(
    X_train_seqval, y_train_seqval, test_size=0.25, random_state=42, stratify=y_train_seqval
)

print("Train seq size:", X_train_seq.shape[0])
print("Val seq size:",   X_val_seq.shape[0])
print("Test seq size:",  X_test_seq.shape[0])

# 4) Convert to torch tensors with shape (N, T, K, F=1)
import torch

def to_seq_tensor(X_np):
    return torch.tensor(X_np, dtype=torch.float32).unsqueeze(-1)  # (N, T, K, 1)

X_train_seq_t = to_seq_tensor(X_train_seq)
X_val_seq_t   = to_seq_tensor(X_val_seq)
X_test_seq_t  = to_seq_tensor(X_test_seq)

y_train_seq_t = torch.tensor(y_train_seq, dtype=torch.long)
y_val_seq_t   = torch.tensor(y_val_seq, dtype=torch.long)
y_test_seq_t  = torch.tensor(y_test_seq, dtype=torch.long)

print("X_train_seq_t shape (N, T, K, F):", X_train_seq_t.shape)
print("X_val_seq_t shape:", X_val_seq_t.shape)
print("X_test_seq_t shape:", X_test_seq_t.shape)

# === Method 3: Temporal GNN (GCN + GRU, eeg-gnn-ssl inspired) ===

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Make sure adjacency is on the right device
A_temporal = A_g.to(device)  # (K, K), same 11-node ECGN

# Move sequence tensors to device
X_train_seq_t = X_train_seq_t.to(device)  # (N_train, T, K, 1)
X_val_seq_t   = X_val_seq_t.to(device)
X_test_seq_t  = X_test_seq_t.to(device)

y_train_seq_t = y_train_seq_t.to(device)
y_val_seq_t   = y_val_seq_t.to(device)
y_test_seq_t  = y_test_seq_t.to(device)

# DataLoaders
batch_size = 64
train_seq_ds = TensorDataset(X_train_seq_t, y_train_seq_t)
val_seq_ds   = TensorDataset(X_val_seq_t,   y_val_seq_t)
test_seq_ds  = TensorDataset(X_test_seq_t,  y_test_seq_t)

train_seq_loader = DataLoader(train_seq_ds, batch_size=batch_size, shuffle=True)
val_seq_loader   = DataLoader(val_seq_ds,   batch_size=batch_size, shuffle=False)
test_seq_loader  = DataLoader(test_seq_ds,  batch_size=batch_size, shuffle=False)


class TemporalGCNGRU(nn.Module):
    """
    Temporal GNN model:
    - GCN over nodes at each time step (using ECGN adjacency)
    - mean pool over nodes -> per-time-step embedding
    - GRU over time
    - final linear classifier on last GRU hidden state
    Inspired by temporal GNN idea in eeg-gnn-ssl, adapted to Fitbit sequences.
    """
    def __init__(self, in_dim, hidden_gcn, hidden_rnn, num_classes):
        super().__init__()
        self.fc_gcn = nn.Linear(in_dim, hidden_gcn)
        self.gru = nn.GRU(input_size=hidden_gcn, hidden_size=hidden_rnn, batch_first=True)
        self.out = nn.Linear(hidden_rnn, num_classes)

    def forward(self, X, A):
        # X: (B, T, N, F)
        B, T, N, F = X.shape

        # Merge batch and time for graph conv: (B*T, N, F)
        X_bt = X.view(B * T, N, F)

        # Graph conv: A @ X_bt  -> (B*T, N, F)
        H = torch.matmul(A, X_bt)    # (N,N) x (B*T,N,F) -> (B*T,N,F)
        H = self.fc_gcn(H)           # (B*T, N, hidden_gcn)
        H = torch.relu(H)

        # Reshape back to (B, T, N, hidden_gcn)
        H = H.view(B, T, N, -1)

        # Mean over nodes: (B, T, hidden_gcn)
        H_time = H.mean(dim=2)

        # GRU over time
        outputs, h_last = self.gru(H_time)  # h_last: (1, B, hidden_rnn)

        # Use last hidden state for classification
        h_last = h_last.squeeze(0)         # (B, hidden_rnn)
        logits = self.out(h_last)          # (B, num_classes)
        return logits


# Model, loss, optimizer
in_dim = 1
hidden_gcn = 32
hidden_rnn = 32
num_classes = 2

temp_gnn = TemporalGCNGRU(in_dim, hidden_gcn, hidden_rnn, num_classes).to(device)
criterion_m3 = nn.CrossEntropyLoss()
optimizer_m3 = optim.Adam(temp_gnn.parameters(), lr=1e-3)


def run_epoch_m3(model, loader, train=True):
    if train:
        model.train()
    else:
        model.eval()

    all_preds = []
    all_labels = []
    total_loss = 0.0

    with torch.set_grad_enabled(train):
        for X_batch, y_batch in loader:
            logits = model(X_batch, A_temporal)
            loss = criterion_m3(logits, y_batch)

            if train:
                optimizer_m3.zero_grad()
                loss.backward()
                optimizer_m3.step()

            total_loss += loss.item() * X_batch.size(0)

            preds = logits.argmax(dim=1)
            all_preds.append(preds.detach().cpu())
            all_labels.append(y_batch.detach().cpu())

    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    avg_loss = total_loss / len(loader.dataset)
    return avg_loss, acc, f1


# Train
num_epochs_m3 = 50
best_val_f1_m3 = 0.0
best_state_m3 = None

for epoch in range(1, num_epochs_m3 + 1):
    train_loss, train_acc, train_f1 = run_epoch_m3(temp_gnn, train_seq_loader, train=True)
    val_loss, val_acc, val_f1 = run_epoch_m3(temp_gnn, val_seq_loader, train=False)

    if val_f1 > best_val_f1_m3:
        best_val_f1_m3 = val_f1
        best_state_m3 = temp_gnn.state_dict()

    if epoch % 10 == 0 or epoch == 1:
        print(
            f"Epoch {epoch:02d} | "
            f"Train loss {train_loss:.3f}, acc {train_acc:.3f}, f1 {train_f1:.3f} | "
            f"Val loss {val_loss:.3f}, acc {val_acc:.3f}, f1 {val_f1:.3f}"
        )

# Load best model
if best_state_m3 is not None:
    temp_gnn.load_state_dict(best_state_m3)


def eval_loader_m3(name, loader):
    loss, acc, f1 = run_epoch_m3(temp_gnn, loader, train=False)
    print(f"{name} loss {loss:.3f}, acc {acc:.3f}, f1 {f1:.3f}")


print("\n=== Method 3: Temporal GCN+GRU on ECGN (final performance) ===")
eval_loader_m3("Train", train_seq_loader)
eval_loader_m3("Val",   val_seq_loader)
eval_loader_m3("Test",  test_seq_loader)

# === Method 4: Attention / Dynamic GNN on ECGN (DGCN / A-GCL inspired) ===

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# We reuse X_train_g, X_val_g, X_test_g, y_train_t, y_val_t, y_test_t from Method 2
X_train_m4 = X_train_g.to(device)  # (N_train, 11, 1)
X_val_m4   = X_val_g.to(device)
X_test_m4  = X_test_g.to(device)

y_train_m4 = y_train_t.to(device)
y_val_m4   = y_val_t.to(device)
y_test_m4  = y_test_t.to(device)

num_nodes = X_train_m4.size(1)
in_dim = X_train_m4.size(2)

print("num_nodes:", num_nodes, "in_dim:", in_dim)

# ECGN mask: 1 where an edge exists, 0 otherwise (based on A_m2 > 0)
A_np = A_m2  # original (non-normalized) adjacency from Method 2
mask = (A_np > 0).astype(np.float32)
mask_torch = torch.tensor(mask, dtype=torch.float32, device=device)

class AttnGCN(nn.Module):
    """
    Attention-based GNN:
    - Learn a dense NxN attention matrix, but masked by ECGN adjacency.
    - Row-wise softmax gives dynamic normalized connectivity.
    - Then do two graph conv layers with this learned adjacency.
    Inspired by DGCN / A-GCL: connectivity is not fixed, but learned from data.
    """
    def __init__(self, in_dim, hidden_dim, num_classes, num_nodes):
        super().__init__()
        self.num_nodes = num_nodes

        # Learnable attention scores matrix (N x N)
        self.attn_scores = nn.Parameter(torch.randn(num_nodes, num_nodes))

        # GCN-like linear layers
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, num_classes)

    def forward(self, X, mask):
        # X: (B, N, F)
        B, N, F = X.shape

        # Compute attention-based adjacency A_attn (N x N)
        # Large negative where mask = 0 to kill those edges
        scores = self.attn_scores.masked_fill(mask == 0, -1e9)
        A_attn = torch.softmax(scores, dim=-1)  # row-normalized dynamic adjacency

        # First graph conv
        H = torch.matmul(A_attn, X)   # (N,N) x (B,N,F) -> (B,N,F)
        H = self.fc1(H)               # (B,N,H)
        H = torch.relu(H)

        # Second graph conv
        H = torch.matmul(A_attn, H)   # (B,N,H)
        H = self.fc2(H)               # (B,N,H)
        H = torch.relu(H)

        # Global mean pooling over nodes
        H_pool = H.mean(dim=1)        # (B,H)

        logits = self.out(H_pool)     # (B,num_classes)
        return logits

# DataLoaders for Method 4
batch_size = 64
train_ds_m4 = TensorDataset(X_train_m4, y_train_m4)
val_ds_m4   = TensorDataset(X_val_m4,   y_val_m4)
test_ds_m4  = TensorDataset(X_test_m4,  y_test_m4)

train_loader_m4 = DataLoader(train_ds_m4, batch_size=batch_size, shuffle=True)
val_loader_m4   = DataLoader(val_ds_m4,   batch_size=batch_size, shuffle=False)
test_loader_m4  = DataLoader(test_ds_m4,  batch_size=batch_size, shuffle=False)

# Model, loss, optimizer
hidden_dim_m4 = 32
num_classes = 2

attn_gnn = AttnGCN(in_dim=in_dim, hidden_dim=hidden_dim_m4,
                   num_classes=num_classes, num_nodes=num_nodes).to(device)

criterion_m4 = nn.CrossEntropyLoss()
optimizer_m4 = optim.Adam(attn_gnn.parameters(), lr=1e-3)

def run_epoch_m4(model, loader, train=True):
    if train:
        model.train()
    else:
        model.eval()

    all_preds = []
    all_labels = []
    total_loss = 0.0

    with torch.set_grad_enabled(train):
        for X_batch, y_batch in loader:
            logits = model(X_batch, mask_torch)
            loss = criterion_m4(logits, y_batch)

            if train:
                optimizer_m4.zero_grad()
                loss.backward()
                optimizer_m4.step()

            total_loss += loss.item() * X_batch.size(0)
            preds = logits.argmax(dim=1)
            all_preds.append(preds.detach().cpu())
            all_labels.append(y_batch.detach().cpu())

    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    avg_loss = total_loss / len(loader.dataset)
    return avg_loss, acc, f1

# Train
num_epochs_m4 = 50
best_val_f1_m4 = 0.0
best_state_m4 = None

for epoch in range(1, num_epochs_m4 + 1):
    train_loss, train_acc, train_f1 = run_epoch_m4(attn_gnn, train_loader_m4, train=True)
    val_loss, val_acc, val_f1 = run_epoch_m4(attn_gnn, val_loader_m4, train=False)

    if val_f1 > best_val_f1_m4:
        best_val_f1_m4 = val_f1
        best_state_m4 = attn_gnn.state_dict()

    if epoch % 10 == 0 or epoch == 1:
        print(
            f"Epoch {epoch:02d} | "
            f"Train loss {train_loss:.3f}, acc {train_acc:.3f}, f1 {train_f1:.3f} | "
            f"Val loss {val_loss:.3f}, acc {val_acc:.3f}, f1 {val_f1:.3f}"
        )

# Load best state
if best_state_m4 is not None:
    attn_gnn.load_state_dict(best_state_m4)

def eval_loader_m4(name, loader):
    loss, acc, f1 = run_epoch_m4(attn_gnn, loader, train=False)
    print(f"{name} loss {loss:.3f}, acc {acc:.3f}, f1 {f1:.3f}")

print("\n=== Method 4: Attention / Dynamic GNN on ECGN (final performance) ===")
eval_loader_m4("Train", train_loader_m4)
eval_loader_m4("Val",   val_loader_m4)
eval_loader_m4("Test",  test_loader_m4)

# === Method 5: LSTM baseline on 7-day sequences (EEG-DL inspired) ===

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# 1) Prepare data: squeeze last feature dim -> (N, T, K)
X_train_lstm = X_train_seq_t.squeeze(-1)  # (N_train, 7, 11)
X_val_lstm   = X_val_seq_t.squeeze(-1)    # (N_val, 7, 11)
X_test_lstm  = X_test_seq_t.squeeze(-1)   # (N_test, 7, 11)

y_train_lstm = y_train_seq_t
y_val_lstm   = y_val_seq_t
y_test_lstm  = y_test_seq_t

print("X_train_lstm shape:", X_train_lstm.shape)
print("X_val_lstm shape:", X_val_lstm.shape)
print("X_test_lstm shape:", X_test_lstm.shape)

# DataLoaders
batch_size = 64
train_ds_m5 = TensorDataset(X_train_lstm, y_train_lstm)
val_ds_m5   = TensorDataset(X_val_lstm,   y_val_lstm)
test_ds_m5  = TensorDataset(X_test_lstm,  y_test_lstm)

train_loader_m5 = DataLoader(train_ds_m5, batch_size=batch_size, shuffle=True)
val_loader_m5   = DataLoader(val_ds_m5,   batch_size=batch_size, shuffle=False)
test_loader_m5  = DataLoader(test_ds_m5,  batch_size=batch_size, shuffle=False)

class LSTMBaseline(nn.Module):
    """
    LSTM over 7-day multivariate Fitbit sequences (no graph).
    Inspired by EEG RNN baselines in SuperBruceJia/EEG-DL.
    """
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=False,
        )
        self.out = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        # x: (B, T, K)
        outputs, (h_n, c_n) = self.lstm(x)  # h_n: (num_layers, B, hidden_size)
        h_last = h_n[-1]  # last layer's hidden state: (B, hidden_size)
        logits = self.out(h_last)
        return logits

input_size = X_train_lstm.size(2)  # K = 11
hidden_size = 32
num_layers = 1
num_classes = 2

lstm_model = LSTMBaseline(input_size, hidden_size, num_layers, num_classes).to(device)
criterion_m5 = nn.CrossEntropyLoss()
optimizer_m5 = optim.Adam(lstm_model.parameters(), lr=1e-3)

def run_epoch_m5(model, loader, train=True):
    if train:
        model.train()
    else:
        model.eval()

    total_loss = 0.0
    all_preds = []
    all_labels = []

    with torch.set_grad_enabled(train):
        for X_batch, y_batch in loader:
            logits = model(X_batch)
            loss = criterion_m5(logits, y_batch)

            if train:
                optimizer_m5.zero_grad()
                loss.backward()
                optimizer_m5.step()

            total_loss += loss.item() * X_batch.size(0)
            preds = logits.argmax(dim=1)
            all_preds.append(preds.detach().cpu())
            all_labels.append(y_batch.detach().cpu())

    all_preds = torch.cat(all_preds).numpy()
    all_labels = torch.cat(all_labels).numpy()

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    avg_loss = total_loss / len(loader.dataset)
    return avg_loss, acc, f1

# Train
num_epochs_m5 = 50
best_val_f1_m5 = 0.0
best_state_m5 = None

for epoch in range(1, num_epochs_m5 + 1):
    train_loss, train_acc, train_f1 = run_epoch_m5(lstm_model, train_loader_m5, train=True)
    val_loss, val_acc, val_f1 = run_epoch_m5(lstm_model, val_loader_m5, train=False)

    if val_f1 > best_val_f1_m5:
        best_val_f1_m5 = val_f1
        best_state_m5 = lstm_model.state_dict()

    if epoch % 10 == 0 or epoch == 1:
        print(
            f"Epoch {epoch:02d} | "
            f"Train loss {train_loss:.3f}, acc {train_acc:.3f}, f1 {train_f1:.3f} | "
            f"Val loss {val_loss:.3f}, acc {val_acc:.3f}, f1 {val_f1:.3f}"
        )

# Load best weights
if best_state_m5 is not None:
    lstm_model.load_state_dict(best_state_m5)

def eval_loader_m5(name, loader):
    loss, acc, f1 = run_epoch_m5(lstm_model, loader, train=False)
    print(f"{name} loss {loss:.3f}, acc {acc:.3f}, f1 {f1:.3f}")

print("\n=== Method 5: LSTM baseline on 7-day sequences (final performance) ===")
eval_loader_m5("Train", train_loader_m5)
eval_loader_m5("Val",   val_loader_m5)
eval_loader_m5("Test",  test_loader_m5)